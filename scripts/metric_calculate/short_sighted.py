#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç®¡ç†å±‚çŸ­è§†æŒ‡æ ‡è®¡ç®—æ¨¡å—

æœ¬æ¨¡å—ç”¨äºè®¡ç®—ä¸Šå¸‚å…¬å¸å¹´æŠ¥ç®¡ç†å±‚æ–‡æœ¬çš„çŸ­è§†æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºäºTF-IDFçš„å•ä¸ªçŸ­è§†è¯è¯­æŒ‡æ ‡ï¼ˆ43ä¸ªï¼‰
2. ç»¼åˆçŸ­è§†æŒ‡æ ‡ï¼ˆ1ä¸ªï¼‰

å®ç°æ–¹æ³•ï¼š
- é‡‡ç”¨TF-IDFæ–¹æ³•å¯¹æ¯ä¸ªçŸ­è§†è¯è¯­è¿›è¡Œé‡åŒ–è¡¨ç¤º
- ä¸ºé¿å…è¯é¢‘ä¸º0å¯¼è‡´åˆ†å­ä¸º0ï¼Œè®¡ç®—æ—¶å°†æ‰€æœ‰è¯è¯­è¯é¢‘åŠ 1
- è®¡ç®—å…¨éƒ¨çŸ­è§†è¯æ±‡æ€»è¯é¢‘å æ–‡æœ¬å…¨éƒ¨è¯é¢‘çš„æ¯”ä¾‹ä½œä¸ºç»¼åˆæŒ‡æ ‡
"""

import os
import re
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import Counter
from math import log
from tqdm import tqdm

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils import TextPreprocessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ShortSightedCalculator:
    """ç®¡ç†å±‚çŸ­è§†æŒ‡æ ‡è®¡ç®—å™¨
    
    è´Ÿè´£è®¡ç®—åŸºäºTF-IDFçš„çŸ­è§†è¯è¯­æŒ‡æ ‡å’Œç»¼åˆçŸ­è§†æŒ‡æ ‡ã€‚
    """
    
    def __init__(self, 
                 short_sighted_dict_path: str = "data/original_data/short_sighted/dict.csv",
                 stopwords_path: str = "data/processed_data/stop_words.txt",
                 input_data_path: str = "data/processed_data/origin_with_textmetric.csv",
                 output_data_path: str = "data/processed_data/origin_with_textmetric_short_sighted.csv"):
        """åˆå§‹åŒ–çŸ­è§†æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            short_sighted_dict_path (str): çŸ­è§†è¯å…¸æ–‡ä»¶è·¯å¾„
            stopwords_path (str): åœç”¨è¯æ–‡ä»¶è·¯å¾„
            input_data_path (str): è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
            output_data_path (str): è¾“å‡ºæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.short_sighted_dict_path = Path(short_sighted_dict_path)
        self.stopwords_path = Path(stopwords_path)
        self.input_data_path = Path(input_data_path)
        self.output_data_path = Path(output_data_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.preprocessor: Optional[TextPreprocessor] = None
        self.short_sighted_words: List[str] = []
        self.data: Optional[pd.DataFrame] = None
        self.management_texts: Dict[Tuple[str, int], str] = {}
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_data_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info("ç®¡ç†å±‚çŸ­è§†æŒ‡æ ‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_components(self) -> None:
        """åˆå§‹åŒ–è®¡ç®—ç»„ä»¶"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–è®¡ç®—ç»„ä»¶...")
            
            # åˆå§‹åŒ–æ–‡æœ¬é¢„å¤„ç†å™¨
            self.preprocessor = TextPreprocessor(str(self.stopwords_path))
            logger.info("âœ… æ–‡æœ¬é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            logger.info("ğŸ‰ æ‰€æœ‰è®¡ç®—ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è®¡ç®—ç»„ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def load_short_sighted_dict(self) -> List[str]:
        """åŠ è½½çŸ­è§†è¯å…¸
        
        Returns:
            List[str]: çŸ­è§†è¯æ±‡åˆ—è¡¨
        """
        try:
            logger.info(f"æ­£åœ¨åŠ è½½çŸ­è§†è¯å…¸: {self.short_sighted_dict_path}")
            
            if not self.short_sighted_dict_path.exists():
                raise FileNotFoundError(f"çŸ­è§†è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {self.short_sighted_dict_path}")
            
            # è¯»å–CSVæ–‡ä»¶
            dict_df = pd.read_csv(self.short_sighted_dict_path, encoding='utf-8')
            
            # æ£€æŸ¥åˆ—å
            if 'short_sighted' not in dict_df.columns:
                raise ValueError(f"çŸ­è§†è¯å…¸æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'short_sighted' åˆ—")
            
            # æå–çŸ­è§†è¯æ±‡ï¼Œå»é™¤ç©ºå€¼
            self.short_sighted_words = dict_df['short_sighted'].dropna().tolist()
            
            # æ¸…ç†è¯æ±‡ï¼ˆå»é™¤ç©ºç™½å­—ç¬¦ï¼‰
            self.short_sighted_words = [word.strip() for word in self.short_sighted_words if word.strip()]
            
            logger.info(f"âœ… çŸ­è§†è¯å…¸åŠ è½½å®Œæˆï¼Œå…± {len(self.short_sighted_words)} ä¸ªè¯æ±‡")
            logger.info(f"çŸ­è§†è¯æ±‡æ ·ä¾‹: {self.short_sighted_words[:10]}")
            
            return self.short_sighted_words
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çŸ­è§†è¯å…¸æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def load_input_data(self) -> pd.DataFrame:
        """åŠ è½½è¾“å…¥æ•°æ®
        
        Returns:
            pd.DataFrame: è¾“å…¥æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨åŠ è½½è¾“å…¥æ•°æ®: {self.input_data_path}")
            
            if not self.input_data_path.exists():
                raise FileNotFoundError(f"è¾“å…¥æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.input_data_path}")
            
            # è¯»å–CSVæ–‡ä»¶
            self.data = pd.read_csv(self.input_data_path, encoding='utf-8')
            
            logger.info(f"âœ… è¾“å…¥æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.data)} è¡Œï¼Œ{len(self.data.columns)} åˆ—")
            
            return self.data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è¾“å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def load_management_texts(self) -> Dict[Tuple[str, int], str]:
        """åŠ è½½ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
        
        Returns:
            Dict[Tuple[str, int], str]: ä»¥(è‚¡ç¥¨ä»£ç , å¹´ä»½)ä¸ºé”®çš„ç®¡ç†å±‚æ–‡æœ¬å­—å…¸
        """
        try:
            logger.info("å¼€å§‹åŠ è½½ç®¡ç†å±‚æ–‡æœ¬æ•°æ®...")
            
            mgmt_file_path = Path("data/original_data/text_data/2001-2020å¹´ä¸­å›½ä¸Šå¸‚å…¬å¸.ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ.å¹´æŠ¥æ–‡æœ¬.xlsx")
            if not mgmt_file_path.exists():
                logger.error(f"ç®¡ç†å±‚æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {mgmt_file_path}")
                return {}
            
            # è¯»å–ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
            mgmt_data = pd.read_excel(mgmt_file_path, engine='openpyxl')
            logger.info(f"ç®¡ç†å±‚æ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(mgmt_data)} è¡Œ")
            
            # è¿‡æ»¤2001-2020å¹´çš„æ•°æ®
            valid_data = mgmt_data[(mgmt_data['ä¼šè®¡å¹´åº¦'] >= 2001) & (mgmt_data['ä¼šè®¡å¹´åº¦'] <= 2020)]
            valid_data = valid_data.dropna(subset=['è‚¡ç¥¨ä»£ç ', 'ä¼šè®¡å¹´åº¦', 'ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'])
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str).str.strip() != '']
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str) != 'nan']
            
            logger.info(f"æœ‰æ•ˆçš„ç®¡ç†å±‚æ–‡æœ¬æ•°æ®: {len(valid_data)} æ¡")
            
            # æ•´ç†æ–‡æœ¬æ•°æ®
            with tqdm(enumerate(valid_data.iterrows()), 
                     desc="æ•´ç†ç®¡ç†å±‚æ–‡æœ¬", 
                     unit="æ¡", 
                     total=len(valid_data)) as pbar:
                
                for row_num, (idx, row) in pbar:
                    try:
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        year = int(row['ä¼šè®¡å¹´åº¦'])
                        text_content = str(row['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹']).strip()
                        
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        self.management_texts[(stock_code, year)] = text_content
                        
                    except Exception as e:
                        logger.error(f"æ•´ç†ç®¡ç†å±‚æ–‡æœ¬ç¬¬ {row_num + 1} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ğŸ‰ ç®¡ç†å±‚æ–‡æœ¬æ•´ç†å®Œæˆï¼Œå…± {len(self.management_texts)} æ¡è®°å½•")
            return self.management_texts
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç®¡ç†å±‚æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def calculate_tf_idf_for_documents(self, 
                                     documents: Dict[Tuple[str, int], str]) -> Dict[Tuple[str, int], Dict[str, float]]:
        """è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„TF-IDFæŒ‡æ ‡
        
        Args:
            documents (Dict[Tuple[str, int], str]): æ–‡æ¡£å­—å…¸
            
        Returns:
            Dict[Tuple[str, int], Dict[str, float]]: TF-IDFæŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹è®¡ç®—TF-IDFæŒ‡æ ‡...")
            
            if self.preprocessor is None:
                raise ValueError("æ–‡æœ¬é¢„å¤„ç†å™¨æœªåˆå§‹åŒ–")
            
            # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼Œè·å–è¯é¢‘ç»Ÿè®¡
            logger.info("ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†æ–‡æ¡£å¹¶ç»Ÿè®¡è¯é¢‘...")
            document_word_counts = {}  # æ¯ä¸ªæ–‡æ¡£çš„è¯é¢‘ç»Ÿè®¡
            document_total_words = {}  # æ¯ä¸ªæ–‡æ¡£çš„æ€»è¯æ•°
            global_word_doc_counts = Counter()  # å…¨å±€è¯æ±‡åœ¨å¤šå°‘ä¸ªæ–‡æ¡£ä¸­å‡ºç°
            
            with tqdm(documents.items(), desc="é¢„å¤„ç†æ–‡æ¡£", unit="æ–‡æ¡£") as pbar:
                for doc_key, text in pbar:
                    stock_code, year = doc_key
                    pbar.set_postfix(code=stock_code[:6], year=year)
                    
                    # è·å–åˆ†è¯ç»“æœ
                    words = self.preprocessor.get_word_list(text)
                    
                    if not words:
                        document_word_counts[doc_key] = {}
                        document_total_words[doc_key] = 0
                        continue
                    
                    # ç»Ÿè®¡è¯é¢‘
                    word_counts = Counter(words)
                    document_word_counts[doc_key] = word_counts
                    document_total_words[doc_key] = len(words)
                    
                    # ç»Ÿè®¡åŒ…å«æ¯ä¸ªè¯çš„æ–‡æ¡£æ•°é‡
                    for word in word_counts:
                        global_word_doc_counts[word] += 1
            
            logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
            logger.info(f"å…¨å±€è¯æ±‡æ€»æ•°: {len(global_word_doc_counts)}")
            
            # ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„TF-IDFæŒ‡æ ‡
            logger.info("ç¬¬äºŒæ­¥ï¼šè®¡ç®—TF-IDFæŒ‡æ ‡...")
            total_documents = len(documents)
            tf_idf_results = {}
            
            with tqdm(documents.items(), desc="è®¡ç®—TF-IDF", unit="æ–‡æ¡£") as pbar:
                for doc_key, text in pbar:
                    stock_code, year = doc_key
                    pbar.set_postfix(code=stock_code[:6], year=year)
                    
                    doc_word_counts = document_word_counts.get(doc_key, {})
                    doc_total_words = document_total_words.get(doc_key, 0)
                    
                    if doc_total_words == 0:
                        # ç©ºæ–‡æ¡£ï¼Œæ‰€æœ‰æŒ‡æ ‡ä¸º0
                        tf_idf_results[doc_key] = {word: 0.0 for word in self.short_sighted_words}
                        continue
                    
                    # è®¡ç®—æ¯ä¸ªçŸ­è§†è¯çš„TF-IDF
                    doc_tf_idf = {}
                    
                    for word in self.short_sighted_words:
                        # è®¡ç®—TFï¼ˆè¯é¢‘åŠ 1é¿å…0å€¼ï¼‰
                        word_count = doc_word_counts.get(word, 0)
                        tf = (word_count + 1) / doc_total_words
                        
                        # è®¡ç®—IDF
                        docs_containing_word = global_word_doc_counts.get(word, 0)
                        idf = log(total_documents / (docs_containing_word + 1))
                        
                        # è®¡ç®—TF-IDF
                        tf_idf = tf * idf
                        doc_tf_idf[word] = tf_idf
                    
                    tf_idf_results[doc_key] = doc_tf_idf
            
            logger.info("ğŸ‰ TF-IDFè®¡ç®—å®Œæˆ")
            return tf_idf_results
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—TF-IDFæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def calculate_comprehensive_short_sighted_indicator(self, 
                                                      documents: Dict[Tuple[str, int], str]) -> Dict[Tuple[str, int], float]:
        """è®¡ç®—ç»¼åˆçŸ­è§†æŒ‡æ ‡
        
        Args:
            documents (Dict[Tuple[str, int], str]): æ–‡æ¡£å­—å…¸
            
        Returns:
            Dict[Tuple[str, int], float]: ç»¼åˆçŸ­è§†æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹è®¡ç®—ç»¼åˆçŸ­è§†æŒ‡æ ‡...")
            
            if self.preprocessor is None:
                raise ValueError("æ–‡æœ¬é¢„å¤„ç†å™¨æœªåˆå§‹åŒ–")
            
            comprehensive_indicators = {}
            
            with tqdm(documents.items(), desc="è®¡ç®—ç»¼åˆçŸ­è§†æŒ‡æ ‡", unit="æ–‡æ¡£") as pbar:
                for doc_key, text in pbar:
                    stock_code, year = doc_key
                    pbar.set_postfix(code=stock_code[:6], year=year)
                    
                    # è·å–åˆ†è¯ç»“æœ
                    words = self.preprocessor.get_word_list(text)
                    
                    if not words:
                        comprehensive_indicators[doc_key] = 0.0
                        continue
                    
                    # ç»Ÿè®¡è¯é¢‘
                    word_counts = Counter(words)
                    total_words = len(words)
                    
                    # ç»Ÿè®¡æ‰€æœ‰çŸ­è§†è¯çš„æ€»å‡ºç°æ¬¡æ•°
                    total_short_sighted_count = 0
                    for word in self.short_sighted_words:
                        total_short_sighted_count += word_counts.get(word, 0)
                    
                    # è®¡ç®—ç»¼åˆæŒ‡æ ‡ï¼ˆçŸ­è§†è¯æ€»é¢‘æ¬¡ / æ–‡æœ¬æ€»è¯æ•° * 100ï¼‰
                    comprehensive_indicator = (total_short_sighted_count / total_words) * 100
                    comprehensive_indicators[doc_key] = comprehensive_indicator
            
            logger.info("ğŸ‰ ç»¼åˆçŸ­è§†æŒ‡æ ‡è®¡ç®—å®Œæˆ")
            return comprehensive_indicators
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ç»¼åˆçŸ­è§†æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def merge_short_sighted_indicators_with_data(self, 
                                               tf_idf_results: Dict[Tuple[str, int], Dict[str, float]],
                                               comprehensive_indicators: Dict[Tuple[str, int], float]) -> pd.DataFrame:
        """å°†çŸ­è§†æŒ‡æ ‡ä¸ç°æœ‰æ•°æ®åˆå¹¶
        
        Args:
            tf_idf_results: TF-IDFæŒ‡æ ‡ç»“æœ
            comprehensive_indicators: ç»¼åˆçŸ­è§†æŒ‡æ ‡ç»“æœ
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        try:
            logger.info("å¼€å§‹åˆå¹¶çŸ­è§†æŒ‡æ ‡ä¸ç°æœ‰æ•°æ®...")
            
            if self.data is None:
                raise ValueError("è¾“å…¥æ•°æ®æœªåŠ è½½")
            
            # å¤åˆ¶ç°æœ‰æ•°æ®
            merged_data = self.data.copy()
            
            # åˆå§‹åŒ–çŸ­è§†æŒ‡æ ‡åˆ—ï¼ˆ43ä¸ªå•è¯æŒ‡æ ‡ + 1ä¸ªç»¼åˆæŒ‡æ ‡ï¼‰
            # å•è¯æŒ‡æ ‡åˆ—å
            word_columns = [f"çŸ­è§†_{word}" for word in self.short_sighted_words]
            # ç»¼åˆæŒ‡æ ‡åˆ—å
            comprehensive_column = "çŸ­è§†_ç»¼åˆæŒ‡æ ‡"
            
            # åˆå§‹åŒ–æ‰€æœ‰åˆ—ä¸ºNaN
            for col in word_columns + [comprehensive_column]:
                merged_data[col] = np.nan
            
            # åˆå¹¶æŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶çŸ­è§†æŒ‡æ ‡...")
            matched_count = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶çŸ­è§†æŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # è·å–è‚¡ç¥¨ä»£ç å’Œå¹´ä»½
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        # æŸ¥æ‰¾åŒ¹é…çš„çŸ­è§†æŒ‡æ ‡
                        doc_key = (stock_code, year)
                        
                        # åˆå¹¶TF-IDFæŒ‡æ ‡
                        if doc_key in tf_idf_results:
                            tf_idf_scores = tf_idf_results[doc_key]
                            for i, word in enumerate(self.short_sighted_words):
                                col_name = word_columns[i]
                                merged_data.at[idx, col_name] = tf_idf_scores.get(word, 0.0)
                        
                        # åˆå¹¶ç»¼åˆæŒ‡æ ‡
                        if doc_key in comprehensive_indicators:
                            merged_data.at[idx, comprehensive_column] = comprehensive_indicators[doc_key]
                            matched_count += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶çŸ­è§†æŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"çŸ­è§†æŒ‡æ ‡åŒ¹é…æˆåŠŸ: {matched_count} æ¡è®°å½•")
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            logger.info("åˆå¹¶ç»“æœç»Ÿè®¡:")
            
            # ç»Ÿè®¡å•è¯æŒ‡æ ‡
            for i, word in enumerate(self.short_sighted_words[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªä½œä¸ºç¤ºä¾‹
                col_name = word_columns[i]
                non_na_count = merged_data[col_name].notna().sum()
                total_count = len(merged_data)
                coverage = non_na_count / total_count * 100
                logger.info(f"{col_name}: {non_na_count}/{total_count} ({coverage:.1f}%)")
            
            # ç»Ÿè®¡ç»¼åˆæŒ‡æ ‡
            comp_non_na_count = merged_data[comprehensive_column].notna().sum()
            comp_coverage = comp_non_na_count / len(merged_data) * 100
            logger.info(f"{comprehensive_column}: {comp_non_na_count}/{len(merged_data)} ({comp_coverage:.1f}%)")
            
            logger.info("ğŸ‰ çŸ­è§†æŒ‡æ ‡ä¸ç°æœ‰æ•°æ®åˆå¹¶å®Œæˆ")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶çŸ­è§†æŒ‡æ ‡ä¸ç°æœ‰æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def save_results(self, merged_data: pd.DataFrame) -> None:
        """ä¿å­˜åˆå¹¶åçš„ç»“æœ
        
        Args:
            merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {self.output_data_path}")
            
            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            merged_data.to_csv(self.output_data_path, index=False, encoding='utf-8')
            
            logger.info(f"âœ… ç»“æœä¿å­˜å®Œæˆ")
            logger.info(f"æ–‡ä»¶å¤§å°: {self.output_data_path.stat().st_size / 1024 / 1024:.2f} MB")
            logger.info(f"æ•°æ®å½¢çŠ¶: {merged_data.shape}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def run_complete_analysis(self) -> pd.DataFrame:
        """è¿è¡Œå®Œæ•´çš„çŸ­è§†æŒ‡æ ‡åˆ†ææµç¨‹
        
        Returns:
            pd.DataFrame: åŒ…å«çŸ­è§†æŒ‡æ ‡çš„å®Œæ•´æ•°æ®
        """
        try:
            logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„çŸ­è§†æŒ‡æ ‡åˆ†ææµç¨‹")
            
            # 1. åˆå§‹åŒ–ç»„ä»¶
            self.initialize_components()
            
            # 2. åŠ è½½çŸ­è§†è¯å…¸
            self.load_short_sighted_dict()
            
            # 3. åŠ è½½è¾“å…¥æ•°æ®
            self.load_input_data()
            
            # 4. åŠ è½½ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
            management_texts = self.load_management_texts()
            
            if not management_texts:
                logger.warning("æœªåŠ è½½åˆ°ç®¡ç†å±‚æ–‡æœ¬æ•°æ®ï¼Œå°†åˆ›å»ºç©ºçš„çŸ­è§†æŒ‡æ ‡åˆ—")
                # åˆ›å»ºç©ºçš„æŒ‡æ ‡ç»“æœ
                tf_idf_results = {}
                comprehensive_indicators = {}
            else:
                # 5. è®¡ç®—TF-IDFæŒ‡æ ‡
                tf_idf_results = self.calculate_tf_idf_for_documents(management_texts)
                
                # 6. è®¡ç®—ç»¼åˆçŸ­è§†æŒ‡æ ‡
                comprehensive_indicators = self.calculate_comprehensive_short_sighted_indicator(management_texts)
            
            # 7. åˆå¹¶æŒ‡æ ‡ä¸ç°æœ‰æ•°æ®
            merged_data = self.merge_short_sighted_indicators_with_data(
                tf_idf_results, comprehensive_indicators
            )
            
            # 8. ä¿å­˜ç»“æœ
            self.save_results(merged_data)
            
            logger.info("ğŸ‰ çŸ­è§†æŒ‡æ ‡åˆ†ææµç¨‹å®Œæˆï¼")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ è¿è¡ŒçŸ­è§†æŒ‡æ ‡åˆ†ææµç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise


def main():
    """ä¸»å‡½æ•° - ç”¨äºç›´æ¥è¿è¡Œæ­¤æ¨¡å—æ—¶çš„æµ‹è¯•"""
    try:
        calculator = ShortSightedCalculator()
        result_data = calculator.run_complete_analysis()
        
        print(f"\nğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ:")
        print(f"æ•°æ®å½¢çŠ¶: {result_data.shape}")
        
        # æ˜¾ç¤ºçŸ­è§†æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        short_sighted_columns = [col for col in result_data.columns if col.startswith('çŸ­è§†_')]
        
        print(f"\nçŸ­è§†æŒ‡æ ‡åˆ—ç»Ÿè®¡:")
        print(f"çŸ­è§†æŒ‡æ ‡åˆ—æ•°: {len(short_sighted_columns)}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
        for col in short_sighted_columns[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä½œä¸ºç¤ºä¾‹
            non_na_count = result_data[col].notna().sum()
            mean_val = result_data[col].mean()
            print(f"{col}: {non_na_count} æ¡æœ‰æ•ˆæ•°æ®, å‡å€¼: {mean_val:.6f}")
        
        # æ˜¾ç¤ºç»¼åˆæŒ‡æ ‡
        comp_col = "çŸ­è§†_ç»¼åˆæŒ‡æ ‡"
        if comp_col in result_data.columns:
            comp_non_na_count = result_data[comp_col].notna().sum()
            comp_mean_val = result_data[comp_col].mean()
            print(f"\n{comp_col}: {comp_non_na_count} æ¡æœ‰æ•ˆæ•°æ®, å‡å€¼: {comp_mean_val:.6f}")
            
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()