#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æŒ‡æ ‡è®¡ç®—æ¨¡å—

æœ¬æ¨¡å—ç”¨äºè®¡ç®—å¤®è¡Œã€æ”¿åºœå’Œç®¡ç†å±‚æ–‡æœ¬çš„å„é¡¹æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
1. å‡€è¯­è°ƒ (TONE)
2. è´Ÿè¯­è°ƒ (NTONE)  
3. ç›¸ä¼¼åº¦ (SIMILARITY)
4. å¯è¯»æ€§ (READABILITY)
"""

import os
import re
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime
from tqdm import tqdm

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils import (
    TextPreprocessor,
    BGEVectorizer,
    SimilarityCalculator,
    ReadabilityCalculator,
    SentimentCalculator,
    create_bge_vectorizer,
    create_sentiment_calculator_from_file
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ProvinceMapper:
    """çœä»½åç§°æ˜ å°„å™¨ç±»
    
    è´Ÿè´£åŠ è½½çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨ï¼Œæä¾›çœä»½åç§°è½¬æ¢åŠŸèƒ½ã€‚
    """
    
    def __init__(self, mapping_file_path: str = "data/original_data/text_data/çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨.csv"):
        """åˆå§‹åŒ–çœä»½æ˜ å°„å™¨
        
        Args:
            mapping_file_path (str): çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨æ–‡ä»¶è·¯å¾„
        """
        self.mapping_file_path = Path(mapping_file_path)
        self.short_to_full: Dict[str, str] = {}  # ç®€ç§°åˆ°å…¨ç§°çš„æ˜ å°„
        self.full_to_short: Dict[str, str] = {}  # å…¨ç§°åˆ°ç®€ç§°çš„æ˜ å°„
        self._load_province_mapping()
    
    def _load_province_mapping(self) -> None:
        """åŠ è½½çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨"""
        try:
            if not self.mapping_file_path.exists():
                logger.error(f"çœä»½å¯¹ç…§è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.mapping_file_path}")
                return
            
            # è¯»å–çœä»½å¯¹ç…§è¡¨
            mapping_df = pd.read_csv(self.mapping_file_path, encoding='utf-8')
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['çœä»½ç®€ç§°', 'çœä»½å…¨ç§°']
            missing_columns = [col for col in required_columns if col not in mapping_df.columns]
            
            if missing_columns:
                logger.error(f"çœä»½å¯¹ç…§è¡¨ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                logger.info(f"ç°æœ‰åˆ—å: {list(mapping_df.columns)}")
                return
            
            # å»ºç«‹æ˜ å°„å…³ç³»
            for _, row in mapping_df.iterrows():
                short_name = str(row['çœä»½ç®€ç§°']).strip()
                full_name = str(row['çœä»½å…¨ç§°']).strip()
                
                if short_name and full_name and short_name != 'nan' and full_name != 'nan':
                    self.short_to_full[short_name] = full_name
                    self.full_to_short[full_name] = short_name
            
            logger.info(f"âœ… çœä»½å¯¹ç…§è¡¨åŠ è½½å®Œæˆï¼Œå…± {len(self.short_to_full)} ä¸ªçœä»½æ˜ å°„å…³ç³»")
            logger.info(f"ç¤ºä¾‹æ˜ å°„: {dict(list(self.short_to_full.items())[:3])}")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çœä»½å¯¹ç…§è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def short_to_full_name(self, short_name: str) -> Optional[str]:
        """å°†çœä»½ç®€ç§°è½¬æ¢ä¸ºå…¨ç§°
        
        Args:
            short_name (str): çœä»½ç®€ç§°
            
        Returns:
            Optional[str]: çœä»½å…¨ç§°ï¼Œå¦‚æœæ‰¾ä¸åˆ°åŒ¹é…åˆ™è¿”å›None
        """
        if not short_name:
            return None
        
        short_name = short_name.strip()
        return self.short_to_full.get(short_name)
    
    def full_to_short_name(self, full_name: str) -> Optional[str]:
        """å°†çœä»½å…¨ç§°è½¬æ¢ä¸ºç®€ç§°
        
        Args:
            full_name (str): çœä»½å…¨ç§°
            
        Returns:
            Optional[str]: çœä»½ç®€ç§°ï¼Œå¦‚æœæ‰¾ä¸åˆ°åŒ¹é…åˆ™è¿”å›None
        """
        if not full_name:
            return None
        
        full_name = full_name.strip()
        return self.full_to_short.get(full_name)
    
    def normalize_province_for_matching(self, province_name: str, target_format: str = 'full') -> Optional[str]:
        """æ ‡å‡†åŒ–çœä»½åç§°ç”¨äºåŒ¹é…
        
        Args:
            province_name (str): åŸå§‹çœä»½åç§°
            target_format (str): ç›®æ ‡æ ¼å¼ï¼Œ'full'è¡¨ç¤ºå…¨ç§°ï¼Œ'short'è¡¨ç¤ºç®€ç§°
            
        Returns:
            Optional[str]: æ ‡å‡†åŒ–åçš„çœä»½åç§°ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not province_name:
            return None
        
        province_name = province_name.strip()
        
        if target_format == 'full':
            # è½¬æ¢ä¸ºå…¨ç§°
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å…¨ç§°
            if province_name in self.full_to_short:
                return province_name
            # å°è¯•ä»ç®€ç§°è½¬æ¢
            full_name = self.short_to_full_name(province_name)
            if full_name:
                return full_name
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›åŸåç§°ï¼ˆå¯èƒ½æ˜¯ç‰¹æ®Šæƒ…å†µï¼‰
            logger.warning(f"æœªæ‰¾åˆ°çœä»½ '{province_name}' çš„å…¨ç§°æ˜ å°„")
            return province_name
            
        elif target_format == 'short':
            # è½¬æ¢ä¸ºç®€ç§°
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ç®€ç§°
            if province_name in self.short_to_full:
                return province_name
            # å°è¯•ä»å…¨ç§°è½¬æ¢
            short_name = self.full_to_short_name(province_name)
            if short_name:
                return short_name
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›åŸåç§°
            logger.warning(f"æœªæ‰¾åˆ°çœä»½ '{province_name}' çš„ç®€ç§°æ˜ å°„")
            return province_name
        
        else:
            logger.error(f"ä¸æ”¯æŒçš„ç›®æ ‡æ ¼å¼: {target_format}")
            return None


class TextMetricCalculator:
    """æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨
    
    è´Ÿè´£è®¡ç®—å¤®è¡Œã€æ”¿åºœå’Œç®¡ç†å±‚æ–‡æœ¬çš„å„é¡¹æŒ‡æ ‡ï¼Œå¹¶ä¸æ•°å€¼æ•°æ®è¿›è¡ŒåŒ¹é…ã€‚
    """
    
    def __init__(self, 
                 numeric_data_path: str = "data/original_data/numeric_data/2001-2020å¹´åˆ¶é€ ä¸šæ•°å€¼æ•°æ®.xlsx",
                 emotion_dict_path: str = "data/processed_data/emo_dict.csv",
                 stopwords_path: str = "data/processed_data/stop_words.txt",
                 bge_model_name: str = "BAAI/bge-large-zh-v1.5",
                 province_mapping_path: str = "data/original_data/text_data/çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨.csv"):
        """åˆå§‹åŒ–æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            numeric_data_path (str): æ•°å€¼æ•°æ®æ–‡ä»¶è·¯å¾„
            emotion_dict_path (str): æƒ…æ„Ÿè¯å…¸æ–‡ä»¶è·¯å¾„  
            stopwords_path (str): åœç”¨è¯æ–‡ä»¶è·¯å¾„
            bge_model_name (str): BGEæ¨¡å‹åç§°
            province_mapping_path (str): çœä»½ç®€ç§°å…¨ç§°å¯¹ç…§è¡¨æ–‡ä»¶è·¯å¾„
        """
        self.numeric_data_path = Path(numeric_data_path)
        self.emotion_dict_path = Path(emotion_dict_path)
        self.stopwords_path = Path(stopwords_path)
        self.bge_model_name = bge_model_name
        
        # åˆå§‹åŒ–å·¥å…·ç»„ä»¶
        self.preprocessor: Optional[TextPreprocessor] = None
        self.vectorizer: Optional[BGEVectorizer] = None
        self.sentiment_calculator: Optional[SentimentCalculator] = None
        self.readability_calculator: Optional[ReadabilityCalculator] = None
        self.province_mapper: Optional[ProvinceMapper] = None
        
        # æ•°æ®å­˜å‚¨
        self.numeric_data: Optional[pd.DataFrame] = None
        self.text_metrics_data: Optional[pd.DataFrame] = None
        
        # ç›¸ä¼¼åº¦è®¡ç®—çš„åŸºå‡†æ–‡æœ¬ï¼ˆç”¨äºå¤®è¡Œå’Œæ”¿åºœæ–‡æœ¬ï¼‰
        self.baseline_texts = {
            'central_bank': "å¤®è¡Œå®æ–½ç¨³å¥è´§å¸æ”¿ç­–ï¼Œä¿æŒæµåŠ¨æ€§åˆç†å……è£•ï¼Œæ”¯æŒå®ä½“ç»æµå‘å±•ã€‚",
            'government': "æ”¿åºœåšæŒé«˜è´¨é‡å‘å±•ï¼Œæ·±åŒ–ä¾›ç»™ä¾§ç»“æ„æ€§æ”¹é©ï¼Œæ¨è¿›ç»æµè½¬å‹å‡çº§ã€‚"
        }
        
        # åˆå§‹åŒ–çœä»½æ˜ å°„å™¨
        try:
            self.province_mapper = ProvinceMapper(province_mapping_path)
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–çœä»½æ˜ å°„å™¨å¤±è´¥: {e}")
            self.province_mapper = None
        
        logger.info("æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_components(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰è®¡ç®—ç»„ä»¶"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–è®¡ç®—ç»„ä»¶...")
            
            # åˆå§‹åŒ–æ–‡æœ¬é¢„å¤„ç†å™¨
            self.preprocessor = TextPreprocessor(str(self.stopwords_path))
            logger.info("âœ… æ–‡æœ¬é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–BGEå‘é‡åŒ–å™¨
            self.vectorizer = create_bge_vectorizer(
                model_name=self.bge_model_name,
                normalize_embeddings=True,
                use_instruction=False,
                preprocessor=self.preprocessor
            )
            if self.vectorizer is None:
                raise ValueError("BGEå‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥")
            logger.info("âœ… BGEå‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æƒ…æ„Ÿè®¡ç®—å™¨
            self.sentiment_calculator = create_sentiment_calculator_from_file(
                str(self.emotion_dict_path), 
                self.preprocessor
            )
            if self.sentiment_calculator is None:
                raise ValueError("æƒ…æ„Ÿè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥")
            logger.info("âœ… æƒ…æ„Ÿè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–å¯è¯»æ€§è®¡ç®—å™¨
            self.readability_calculator = ReadabilityCalculator(self.preprocessor)
            logger.info("âœ… å¯è¯»æ€§è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
            
            logger.info("ğŸ‰ æ‰€æœ‰è®¡ç®—ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è®¡ç®—ç»„ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def load_numeric_data(self) -> pd.DataFrame:
        """åŠ è½½æ•°å€¼æ•°æ®
        
        Returns:
            pd.DataFrame: æ•°å€¼æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ•°å€¼æ•°æ®: {self.numeric_data_path}")
            
            if not self.numeric_data_path.exists():
                raise FileNotFoundError(f"æ•°å€¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.numeric_data_path}")
            
            # è¯»å–Excelæ–‡ä»¶
            self.numeric_data = pd.read_excel(self.numeric_data_path, engine='openpyxl')
            
            logger.info(f"âœ… æ•°å€¼æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.numeric_data)} è¡Œ")
            logger.info(f"æ•°æ®åˆ—å: {list(self.numeric_data.columns)}")
            
            return self.numeric_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°å€¼æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def calculate_text_metrics(self, text: str, baseline_text: Optional[str] = None) -> Dict[str, float]:
        """è®¡ç®—å•ä¸ªæ–‡æœ¬çš„æ‰€æœ‰æŒ‡æ ‡
        
        Args:
            text (str): å¾…åˆ†ææ–‡æœ¬
            baseline_text (Optional[str]): ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„åŸºå‡†æ–‡æœ¬
            
        Returns:
            Dict[str, float]: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        try:
            if not text or not text.strip():
                return {
                    'tone': 0.0,
                    'negative_tone': 0.0,
                    'similarity': 0.0,
                    'readability': 0.0
                }
            
            # æ£€æŸ¥è®¡ç®—å™¨æ˜¯å¦å·²åˆå§‹åŒ–
            if self.sentiment_calculator is None:
                logger.error("æƒ…æ„Ÿè®¡ç®—å™¨æœªåˆå§‹åŒ–")
                return {
                    'tone': 0.0,
                    'negative_tone': 0.0,
                    'similarity': 0.0,
                    'readability': 0.0
                }
            
            if self.vectorizer is None:
                logger.error("å‘é‡åŒ–å™¨æœªåˆå§‹åŒ–")
                return {
                    'tone': 0.0,
                    'negative_tone': 0.0,
                    'similarity': 0.0,
                    'readability': 0.0
                }
            
            if self.readability_calculator is None:
                logger.error("å¯è¯»æ€§è®¡ç®—å™¨æœªåˆå§‹åŒ–")
                return {
                    'tone': 0.0,
                    'negative_tone': 0.0,
                    'similarity': 0.0,
                    'readability': 0.0
                }
            
            # è®¡ç®—æƒ…æ„ŸæŒ‡æ ‡
            sentiment_metrics = self.sentiment_calculator.calculate_all_metrics(text)
            tone = sentiment_metrics['tone']
            negative_tone = sentiment_metrics['negative_tone']
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            if baseline_text:
                similarity = SimilarityCalculator.text_similarity(
                    text, baseline_text, self.vectorizer
                )
            else:
                similarity = 0.0
            
            # è®¡ç®—å¯è¯»æ€§ï¼ˆä½¿ç”¨å­—ç¬¦çº§åˆ«ï¼‰
            readability = self.readability_calculator.calculate_readability(
                text, use_character_count=True
            )
            
            return {
                'tone': tone,
                'negative_tone': negative_tone,
                'similarity': similarity,
                'readability': readability
            }
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡æœ¬æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'tone': 0.0,
                'negative_tone': 0.0,
                'similarity': 0.0,
                'readability': 0.0
            }
    
    def process_central_bank_texts(self) -> Dict[int, Dict[str, float]]:
        """å¤„ç†å¤®è¡Œæ–‡æœ¬æ•°æ®
        
        Returns:
            Dict[int, Dict[str, float]]: ä»¥å¹´ä»½ä¸ºé”®çš„å¤®è¡Œæ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†å¤®è¡Œæ–‡æœ¬æ•°æ®...")
            
            central_bank_dir = Path("data/original_data/text_data/å¤®è¡Œæ–‡æœ¬")
            if not central_bank_dir.exists():
                logger.error(f"å¤®è¡Œæ–‡æœ¬ç›®å½•ä¸å­˜åœ¨: {central_bank_dir}")
                return {}
            
            # è·å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
            txt_files = list(central_bank_dir.glob("*.txt"))
            
            # è¿‡æ»¤2001-2020å¹´çš„æ–‡ä»¶
            valid_files = []
            for txt_file in txt_files:
                filename = txt_file.stem
                year_match = re.match(r'(\d{4})ç¬¬å››å­£åº¦', filename)
                if year_match:
                    year = int(year_match.group(1))
                    if 2001 <= year <= 2020:
                        valid_files.append((txt_file, year))
            
            if not valid_files:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¤®è¡Œæ–‡æœ¬æ–‡ä»¶")
                return {}
            
            central_bank_metrics = {}
            baseline_text = self.baseline_texts['central_bank']
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(valid_files, desc="å¤„ç†å¤®è¡Œæ–‡æœ¬", unit="æ–‡ä»¶") as pbar:
                for txt_file, year in pbar:
                    try:
                        pbar.set_postfix(year=year)
                        
                        # è¯»å–æ–‡æœ¬å†…å®¹
                        with open(txt_file, 'r', encoding='utf-8') as f:
                            text_content = f.read().strip()
                        
                        if not text_content:
                            logger.warning(f"å¤®è¡Œæ–‡æœ¬æ–‡ä»¶ä¸ºç©º: {txt_file}")
                            continue
                        
                        # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
                        metrics = self.calculate_text_metrics(text_content, baseline_text)
                        central_bank_metrics[year] = metrics
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†å¤®è¡Œæ–‡æœ¬æ–‡ä»¶ {txt_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ğŸ‰ å¤®è¡Œæ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(central_bank_metrics)} å¹´çš„æ•°æ®")
            return central_bank_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤®è¡Œæ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def process_government_texts(self) -> Dict[Tuple[str, int], Dict[str, float]]:
        """å¤„ç†æ”¿åºœæ–‡æœ¬æ•°æ®
        
        Returns:
            Dict[Tuple[str, int], Dict[str, float]]: ä»¥(çœä»½å…¨ç§°, å¹´ä»½)ä¸ºé”®çš„æ”¿åºœæ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†æ”¿åºœæ–‡æœ¬æ•°æ®...")
            
            gov_file_path = Path("data/original_data/text_data/2001-2022çœçº§å·¥ä½œæ”¿åºœæŠ¥å‘Š.xlsx")
            if not gov_file_path.exists():
                logger.error(f"æ”¿åºœæ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {gov_file_path}")
                return {}
            
            # è¯»å–æ”¿åºœæ–‡æœ¬æ•°æ®
            gov_data = pd.read_excel(gov_file_path, engine='openpyxl')
            logger.info(f"æ”¿åºœæ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(gov_data)} è¡Œ")
            
            # è¿‡æ»¤2001-2020å¹´çš„æ•°æ®
            valid_data = gov_data[(gov_data['ä¼šè®¡å¹´'] >= 2001) & (gov_data['ä¼šè®¡å¹´'] <= 2020)]
            valid_data = valid_data.dropna(subset=['çœä»½åç§°', 'ä¼šè®¡å¹´', 'æ”¿åºœæŠ¥å‘Š'])
            valid_data = valid_data[valid_data['æ”¿åºœæŠ¥å‘Š'].str.strip() != '']
            
            logger.info(f"æœ‰æ•ˆçš„æ”¿åºœæ–‡æœ¬æ•°æ®: {len(valid_data)} æ¡")
            
            government_metrics = {}
            baseline_text = self.baseline_texts['government']
            
            # ç»Ÿè®¡çœä»½è½¬æ¢æƒ…å†µ
            conversion_stats = {'success': 0, 'failed': 0, 'failed_provinces': set()}
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(enumerate(valid_data.iterrows()), 
                     desc="å¤„ç†æ”¿åºœæ–‡æœ¬", 
                     unit="æ¡", 
                     total=len(valid_data)) as pbar:
                
                for row_num, (idx, row) in pbar:
                    try:
                        province_short = str(row['çœä»½åç§°']).strip()  # æ”¿åºœæŠ¥å‘Šä¸­çš„çœä»½ç®€ç§°
                        year = int(row['ä¼šè®¡å¹´'])
                        text_content = str(row['æ”¿åºœæŠ¥å‘Š']).strip()
                        
                        pbar.set_postfix(province=province_short[:4], year=year)
                        
                        if not text_content or text_content == 'nan':
                            continue
                        
                        # ä½¿ç”¨çœä»½æ˜ å°„å™¨å°†ç®€ç§°è½¬æ¢ä¸ºå…¨ç§°
                        province_full = None
                        if self.province_mapper:
                            province_full = self.province_mapper.short_to_full_name(province_short)
                        
                        if not province_full:
                            # å¦‚æœçœä»½æ˜ å°„å™¨ä¸å¯ç”¨æˆ–è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç®€å•çš„è§„åˆ™è½¬æ¢
                            province_full = self._fallback_province_conversion(province_short)
                            conversion_stats['failed'] += 1
                            conversion_stats['failed_provinces'].add(province_short)
                            logger.warning(f"çœä»½ '{province_short}' è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨fallbackè½¬æ¢: '{province_full}'")
                        else:
                            conversion_stats['success'] += 1
                        
                        # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
                        metrics = self.calculate_text_metrics(text_content, baseline_text)
                        
                        # ä½¿ç”¨çœä»½å…¨ç§°ä½œä¸ºé”®
                        government_metrics[(province_full, year)] = metrics
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†æ”¿åºœæ–‡æœ¬ç¬¬ {row_num + 1} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            # è®°å½•çœä»½è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"çœä»½è½¬æ¢ç»Ÿè®¡: æˆåŠŸ {conversion_stats['success']} æ¡, å¤±è´¥ {conversion_stats['failed']} æ¡")
            if conversion_stats['failed_provinces']:
                logger.warning(f"è½¬æ¢å¤±è´¥çš„çœä»½: {sorted(conversion_stats['failed_provinces'])}")
            
            logger.info(f"ğŸ‰ æ”¿åºœæ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(government_metrics)} æ¡æ•°æ®")
            return government_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ”¿åºœæ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def _fallback_province_conversion(self, province_short: str) -> str:
        """å¤‡ç”¨çš„çœä»½è½¬æ¢æ–¹æ³•
        
        å½“çœä»½æ˜ å°„å™¨ä¸å¯ç”¨æ—¶ä½¿ç”¨çš„ç®€å•è½¬æ¢è§„åˆ™ã€‚
        
        Args:
            province_short (str): çœä»½ç®€ç§°
            
        Returns:
            str: çœä»½å…¨ç§°ï¼ˆå°½åŠ›è½¬æ¢ï¼‰
        """
        if not province_short:
            return ""
        
        province_short = province_short.strip()
        
        # ç‰¹æ®Šåœ°åŒºå¤„ç†
        special_regions = {
            'åŒ—äº¬': 'åŒ—äº¬å¸‚',
            'ä¸Šæµ·': 'ä¸Šæµ·å¸‚',
            'å¤©æ´¥': 'å¤©æ´¥å¸‚',
            'é‡åº†': 'é‡åº†å¸‚',
            'å†…è’™å¤': 'å†…è’™å¤è‡ªæ²»åŒº',
            'å¹¿è¥¿': 'å¹¿è¥¿å£®æ—è‡ªæ²»åŒº',
            'è¥¿è—': 'è¥¿è—è‡ªæ²»åŒº',
            'å®å¤': 'å®å¤å›æ—è‡ªæ²»åŒº',
            'æ–°ç–†': 'æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº',
            'é¦™æ¸¯': 'é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº',
            'æ¾³é—¨': 'æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº',
            'å°æ¹¾': 'å°æ¹¾çœ'
        }
        
        if province_short in special_regions:
            return special_regions[province_short]
        
        # ä¸€èˆ¬çœä»½ï¼šå¦‚æœä¸ä»¥"çœ"ç»“å°¾ï¼Œåˆ™æ·»åŠ "çœ"
        if not province_short.endswith('çœ'):
            return province_short + 'çœ'
        
        return province_short
    
    def process_management_texts(self) -> Dict[Tuple[str, int], Dict[str, float]]:
        """å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
        
        è®¡ç®—ç®¡ç†å±‚ç›¸ä¼¼åº¦ï¼šåŒä¸€å…¬å¸æœ¬å¹´åº¦ä¸ä¸Šä¸€å¹´åº¦æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Returns:
            Dict[Tuple[str, int], Dict[str, float]]: ä»¥(è‚¡ç¥¨ä»£ç , å¹´ä»½)ä¸ºé”®çš„ç®¡ç†å±‚æ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ•°æ®...")
            
            mgmt_file_path = Path("data/original_data/text_data/2001-2020å¹´ä¸­å›½ä¸Šå¸‚å…¬å¸.ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ.å¹´æŠ¥æ–‡æœ¬.xlsx")
            if not mgmt_file_path.exists():
                logger.error(f"ç®¡ç†å±‚æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {mgmt_file_path}")
                return {}
            
            # è¯»å–ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
            mgmt_data = pd.read_excel(mgmt_file_path, engine='openpyxl')
            logger.info(f"ç®¡ç†å±‚æ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(mgmt_data)} è¡Œ")
            
            # è¿‡æ»¤2001-2020å¹´çš„æ•°æ®
            valid_data = mgmt_data[(mgmt_data['ä¼šè®¡å¹´åº¦'] >= 2001) & (mgmt_data['ä¼šè®¡å¹´åº¦'] <= 2020)]
            valid_data = valid_data.dropna(subset=['è‚¡ç¥¨ä»£ç ', 'ä¼šè®¡å¹´åº¦', 'ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'])
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str).str.strip() != '']
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str) != 'nan']
            
            logger.info(f"æœ‰æ•ˆçš„ç®¡ç†å±‚æ–‡æœ¬æ•°æ®: {len(valid_data)} æ¡")
            
            # å…ˆæ•´ç†æ•°æ®ï¼ŒæŒ‰å…¬å¸å’Œå¹´ä»½ç»„ç»‡
            logger.info("æ­£åœ¨æ•´ç†æ–‡æœ¬æ•°æ®...")
            company_texts = {}  # {è‚¡ç¥¨ä»£ç : {å¹´ä»½: æ–‡æœ¬å†…å®¹}}
            
            with tqdm(enumerate(valid_data.iterrows()), 
                     desc="æ•´ç†ç®¡ç†å±‚æ–‡æœ¬", 
                     unit="æ¡", 
                     total=len(valid_data)) as pbar:
                
                for row_num, (idx, row) in pbar:
                    try:
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        year = int(row['ä¼šè®¡å¹´åº¦'])
                        text_content = str(row['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹']).strip()
                        
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        if stock_code not in company_texts:
                            company_texts[stock_code] = {}
                        
                        company_texts[stock_code][year] = text_content
                        
                    except Exception as e:
                        logger.error(f"æ•´ç†ç®¡ç†å±‚æ–‡æœ¬ç¬¬ {row_num + 1} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"æ•´ç†å®Œæˆï¼Œå…± {len(company_texts)} å®¶å…¬å¸çš„æ–‡æœ¬æ•°æ®")
            
            # è®¡ç®—æŒ‡æ ‡
            management_metrics = {}
            similarity_count = 0
            
            # è®¡ç®—æ€»çš„å¤„ç†æ¡ç›®æ•°
            total_items = sum(len(year_texts) for year_texts in company_texts.values())
            
            with tqdm(total=total_items, 
                     desc="è®¡ç®—ç®¡ç†å±‚æŒ‡æ ‡", 
                     unit="æ¡") as pbar:
                
                for stock_code, year_texts in company_texts.items():
                    try:
                        for year, current_text in year_texts.items():
                            pbar.set_postfix(code=stock_code[:6], year=year)
                            
                            # è®¡ç®—åŸºæœ¬çš„æƒ…æ„Ÿå’Œå¯è¯»æ€§æŒ‡æ ‡
                            if self.sentiment_calculator is None or self.readability_calculator is None:
                                logger.error("è®¡ç®—å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ­¤æ¡è®°å½•")
                                pbar.update(1)
                                continue
                                
                            sentiment_metrics = self.sentiment_calculator.calculate_all_metrics(current_text)
                            tone = sentiment_metrics['tone']
                            negative_tone = sentiment_metrics['negative_tone']
                            
                            readability = self.readability_calculator.calculate_readability(
                                current_text, use_character_count=True
                            )
                            
                            # è®¡ç®—ä¸å‰ä¸€å¹´çš„ç›¸ä¼¼åº¦
                            similarity = 0.0
                            previous_year = int(year) - 1
                            
                            if previous_year in year_texts and self.vectorizer is not None:
                                previous_text = year_texts[previous_year]
                                if previous_text and previous_text.strip():
                                    similarity = SimilarityCalculator.text_similarity(
                                        current_text, previous_text, self.vectorizer
                                    )
                                    if similarity > 0:
                                        similarity_count += 1
                            
                            # å­˜å‚¨æŒ‡æ ‡
                            management_metrics[(stock_code, year)] = {
                                'tone': tone,
                                'negative_tone': negative_tone,
                                'similarity': similarity,
                                'readability': readability
                            }
                            
                            pbar.update(1)
                            
                    except Exception as e:
                        logger.error(f"å¤„ç†å…¬å¸ {stock_code} ç®¡ç†å±‚æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        # ä»ç„¶æ›´æ–°è¿›åº¦æ¡
                        remaining_items = len(year_texts)
                        pbar.update(remaining_items)
                        continue
            
            logger.info(f"ğŸ‰ ç®¡ç†å±‚æ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(management_metrics)} æ¡æ•°æ®")
            logger.info(f"æˆåŠŸè®¡ç®—ç›¸ä¼¼åº¦çš„è®°å½•æ•°: {similarity_count}/{len(management_metrics)}")
            
            return management_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def merge_text_metrics_with_numeric_data(self, 
                                           central_bank_metrics: Dict[int, Dict[str, float]],
                                           government_metrics: Dict[Tuple[str, int], Dict[str, float]],
                                           management_metrics: Dict[Tuple[str, int], Dict[str, float]]) -> pd.DataFrame:
        """å°†æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®åˆå¹¶
        
        Args:
            central_bank_metrics: å¤®è¡Œæ–‡æœ¬æŒ‡æ ‡
            government_metrics: æ”¿åºœæ–‡æœ¬æŒ‡æ ‡ï¼Œé”®ä¸º(çœä»½å…¨ç§°, å¹´ä»½)
            management_metrics: ç®¡ç†å±‚æ–‡æœ¬æŒ‡æ ‡
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        try:
            logger.info("å¼€å§‹åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®...")
            
            # æ£€æŸ¥æ•°å€¼æ•°æ®æ˜¯å¦å­˜åœ¨
            if self.numeric_data is None:
                raise ValueError("æ•°å€¼æ•°æ®æœªåŠ è½½")
            
            # å¤åˆ¶æ•°å€¼æ•°æ®
            merged_data = self.numeric_data.copy()
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½', 'æ‰€å±çœä»½', 'è‚¡ç¥¨ä»£ç ']
            missing_columns = [col for col in required_columns if col not in merged_data.columns]
            
            if missing_columns:
                logger.error(f"æ•°å€¼æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                logger.info(f"ç°æœ‰åˆ—å: {list(merged_data.columns)}")
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            
            # åˆå§‹åŒ–æ–‡æœ¬æŒ‡æ ‡åˆ—
            text_metric_columns = [
                # å¤®è¡ŒæŒ‡æ ‡
                'å¤®è¡Œ_å‡€è¯­è°ƒ', 'å¤®è¡Œ_è´Ÿè¯­è°ƒ', 'å¤®è¡Œ_ç›¸ä¼¼åº¦', 'å¤®è¡Œ_å¯è¯»æ€§',
                # æ”¿åºœæŒ‡æ ‡
                'æ”¿åºœ_å‡€è¯­è°ƒ', 'æ”¿åºœ_è´Ÿè¯­è°ƒ', 'æ”¿åºœ_ç›¸ä¼¼åº¦', 'æ”¿åºœ_å¯è¯»æ€§',
                # ç®¡ç†å±‚æŒ‡æ ‡
                'ç®¡ç†å±‚_å‡€è¯­è°ƒ', 'ç®¡ç†å±‚_è´Ÿè¯­è°ƒ', 'ç®¡ç†å±‚_ç›¸ä¼¼åº¦', 'ç®¡ç†å±‚_å¯è¯»æ€§'
            ]
            
            for col in text_metric_columns:
                merged_data[col] = np.nan
            
            # åˆå¹¶å¤®è¡ŒæŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶å¤®è¡ŒæŒ‡æ ‡...")
            central_bank_matched = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶å¤®è¡ŒæŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # ä»"ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½"æå–å¹´ä»½
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(year=year)
                        
                        if year in central_bank_metrics:
                            metrics = central_bank_metrics[year]
                            merged_data.at[idx, 'å¤®è¡Œ_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.at[idx, 'å¤®è¡Œ_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.at[idx, 'å¤®è¡Œ_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.at[idx, 'å¤®è¡Œ_å¯è¯»æ€§'] = metrics['readability']
                            central_bank_matched += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶å¤®è¡ŒæŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"å¤®è¡ŒæŒ‡æ ‡åŒ¹é…æˆåŠŸ: {central_bank_matched} æ¡è®°å½•")
            
            # åˆå¹¶æ”¿åºœæŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶æ”¿åºœæŒ‡æ ‡...")
            government_matched = 0
            government_match_failed = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶æ”¿åºœæŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # è·å–çœä»½å’Œå¹´ä»½
                        province_full_from_numeric = str(row['æ‰€å±çœä»½']).strip()  # æ•°å€¼æ•°æ®ä¸­çš„çœä»½å…¨ç§°
                        
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(province=province_full_from_numeric[:4], year=year)
                        
                        # ç›´æ¥ä½¿ç”¨çœä»½å…¨ç§°è¿›è¡ŒåŒ¹é…ï¼ˆæ”¿åºœæŒ‡æ ‡å­—å…¸çš„é”®å·²ç»æ˜¯çœä»½å…¨ç§°ï¼‰
                        gov_key = (province_full_from_numeric, year)
                        
                        if gov_key in government_metrics:
                            metrics = government_metrics[gov_key]
                            merged_data.at[idx, 'æ”¿åºœ_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.at[idx, 'æ”¿åºœ_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.at[idx, 'æ”¿åºœ_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.at[idx, 'æ”¿åºœ_å¯è¯»æ€§'] = metrics['readability']
                            government_matched += 1
                        else:
                            government_match_failed += 1
                            # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä¸€äº›å˜ä½“åŒ¹é…
                            possible_variants = [
                                province_full_from_numeric.replace('çœ', ''),  # å»æ‰"çœ"å­—
                                province_full_from_numeric + 'çœ' if not province_full_from_numeric.endswith('çœ') else province_full_from_numeric,  # æ·»åŠ "çœ"å­—
                            ]
                            
                            matched = False
                            for variant in possible_variants:
                                variant_key = (variant, year)
                                if variant_key in government_metrics:
                                    metrics = government_metrics[variant_key]
                                    merged_data.at[idx, 'æ”¿åºœ_å‡€è¯­è°ƒ'] = metrics['tone']
                                    merged_data.at[idx, 'æ”¿åºœ_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                                    merged_data.at[idx, 'æ”¿åºœ_ç›¸ä¼¼åº¦'] = metrics['similarity']
                                    merged_data.at[idx, 'æ”¿åºœ_å¯è¯»æ€§'] = metrics['readability']
                                    government_matched += 1
                                    government_match_failed -= 1
                                    matched = True
                                    break
                            
                            if not matched:
                                logger.debug(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ”¿åºœæŒ‡æ ‡: {gov_key}")
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶æ”¿åºœæŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"æ”¿åºœæŒ‡æ ‡åŒ¹é…æˆåŠŸ: {government_matched} æ¡è®°å½•")
            logger.info(f"æ”¿åºœæŒ‡æ ‡åŒ¹é…å¤±è´¥: {government_match_failed} æ¡è®°å½•")
            
            # åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡...")
            management_matched = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # è·å–è‚¡ç¥¨ä»£ç å’Œå¹´ä»½
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        # æŸ¥æ‰¾åŒ¹é…çš„ç®¡ç†å±‚æŒ‡æ ‡
                        mgmt_key = (stock_code, year)
                        if mgmt_key in management_metrics:
                            metrics = management_metrics[mgmt_key]
                            merged_data.at[idx, 'ç®¡ç†å±‚_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.at[idx, 'ç®¡ç†å±‚_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.at[idx, 'ç®¡ç†å±‚_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.at[idx, 'ç®¡ç†å±‚_å¯è¯»æ€§'] = metrics['readability']
                            management_matched += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ç®¡ç†å±‚æŒ‡æ ‡åŒ¹é…æˆåŠŸ: {management_matched} æ¡è®°å½•")
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            logger.info("åˆå¹¶ç»“æœç»Ÿè®¡:")
            for col in text_metric_columns:
                non_na_count = merged_data[col].notna().sum()
                total_count = len(merged_data)
                coverage = non_na_count / total_count * 100
                logger.info(f"{col}: {non_na_count}/{total_count} ({coverage:.1f}%)")
            
            # ç‰¹åˆ«ç»Ÿè®¡ç®¡ç†å±‚ç›¸ä¼¼åº¦
            mgmt_similarity_count = merged_data['ç®¡ç†å±‚_ç›¸ä¼¼åº¦'].notna().sum()
            mgmt_similarity_positive = (merged_data['ç®¡ç†å±‚_ç›¸ä¼¼åº¦'] > 0).sum()
            logger.info(f"ç®¡ç†å±‚ç›¸ä¼¼åº¦ç»Ÿè®¡: æ€»è®¡{mgmt_similarity_count}æ¡, å…¶ä¸­{mgmt_similarity_positive}æ¡ç›¸ä¼¼åº¦>0")
            
            logger.info("ğŸ‰ æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®åˆå¹¶å®Œæˆ")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def save_results(self, merged_data: pd.DataFrame, 
                    output_path: str = "data/processed_data/origin_with_textmetric.csv") -> None:
        """ä¿å­˜åˆå¹¶åçš„ç»“æœ
        
        Args:
            merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_path}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path_obj = Path(output_path)
            output_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            merged_data.to_csv(output_path_obj, index=False, encoding='utf-8')
            
            logger.info(f"âœ… ç»“æœä¿å­˜å®Œæˆ")
            logger.info(f"æ–‡ä»¶å¤§å°: {output_path_obj.stat().st_size / 1024 / 1024:.2f} MB")
            logger.info(f"æ•°æ®å½¢çŠ¶: {merged_data.shape}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def run_complete_analysis(self) -> pd.DataFrame:
        """è¿è¡Œå®Œæ•´çš„æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹
        
        Returns:
            pd.DataFrame: åŒ…å«æ–‡æœ¬æŒ‡æ ‡çš„å®Œæ•´æ•°æ®
        """
        try:
            logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹")
            
            # 1. åˆå§‹åŒ–ç»„ä»¶
            self.initialize_components()
            
            # 2. åŠ è½½æ•°å€¼æ•°æ®
            self.load_numeric_data()
            
            # 3. å¤„ç†å„ç±»æ–‡æœ¬æ•°æ®
            central_bank_metrics = self.process_central_bank_texts()
            government_metrics = self.process_government_texts()
            management_metrics = self.process_management_texts()
            
            # 4. åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®
            merged_data = self.merge_text_metrics_with_numeric_data(
                central_bank_metrics, government_metrics, management_metrics
            )
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(merged_data)
            
            logger.info("ğŸ‰ æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹å®Œæˆï¼")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ è¿è¡Œæ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise


def main():
    """ä¸»å‡½æ•° - ç”¨äºç›´æ¥è¿è¡Œæ­¤æ¨¡å—æ—¶çš„æµ‹è¯•"""
    try:
        calculator = TextMetricCalculator()
        result_data = calculator.run_complete_analysis()
        
        print(f"\nğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ:")
        print(f"æ•°æ®å½¢çŠ¶: {result_data.shape}")
        print(f"\nåˆ—å: {list(result_data.columns)}")
        
        # æ˜¾ç¤ºæ–‡æœ¬æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        text_columns = [col for col in result_data.columns 
                       if any(keyword in col for keyword in ['å¤®è¡Œ', 'æ”¿åºœ', 'ç®¡ç†å±‚'])]
        
        print(f"\næ–‡æœ¬æŒ‡æ ‡åˆ—ç»Ÿè®¡:")
        for col in text_columns:
            non_na_count = result_data[col].notna().sum()
            mean_val = result_data[col].mean()
            print(f"{col}: {non_na_count} æ¡æœ‰æ•ˆæ•°æ®, å‡å€¼: {mean_val:.4f}")
            
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()