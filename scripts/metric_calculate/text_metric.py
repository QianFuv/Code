#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æŒ‡æ ‡è®¡ç®—æ¨¡å—

æœ¬æ¨¡å—ç”¨äºè®¡ç®—å¤®è¡Œã€æ”¿åºœå’Œç®¡ç†å±‚æ–‡æœ¬çš„å„é¡¹æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
1. å‡€è¯­è°ƒ (TONE)
2. è´Ÿè¯­è°ƒ (NTONE)  
3. ç›¸ä¼¼åº¦ (SIMILARITY)
4. å¯è¯»æ€§ (READABILITY)

å¤„ç†çš„æ–‡æœ¬ç±»å‹ï¼š
- å¤®è¡Œæ–‡æœ¬ï¼šå­£åº¦è´§å¸æ”¿ç­–æ‰§è¡ŒæŠ¥å‘Š
- æ”¿åºœæ–‡æœ¬ï¼šçœçº§æ”¿åºœå·¥ä½œæŠ¥å‘Š
- ç®¡ç†å±‚æ–‡æœ¬ï¼šä¸Šå¸‚å…¬å¸ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ

ä½œè€…ï¼šæ ¹æ®è®ºæ–‡è¦æ±‚å®ç°
æ—¥æœŸï¼š2025å¹´
"""

import os
import re
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from tqdm import tqdm

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils import (
    TextPreprocessor,
    BGEVectorizer,
    SimilarityCalculator,
    ReadabilityCalculator,
    create_bge_vectorizer,
    create_sentiment_calculator_from_file
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TextMetricCalculator:
    """æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨
    
    è´Ÿè´£è®¡ç®—å¤®è¡Œã€æ”¿åºœå’Œç®¡ç†å±‚æ–‡æœ¬çš„å„é¡¹æŒ‡æ ‡ï¼Œå¹¶ä¸æ•°å€¼æ•°æ®è¿›è¡ŒåŒ¹é…ã€‚
    """
    
    def __init__(self, 
                 numeric_data_path: str = "data/original_data/numeric_data/2001-2020å¹´åˆ¶é€ ä¸šæ•°å€¼æ•°æ®.xlsx",
                 emotion_dict_path: str = "data/processed_data/emo_dict.csv",
                 stopwords_path: str = "data/processed_data/stop_words.txt",
                 bge_model_name: str = "BAAI/bge-large-zh-v1.5"):
        """åˆå§‹åŒ–æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            numeric_data_path (str): æ•°å€¼æ•°æ®æ–‡ä»¶è·¯å¾„
            emotion_dict_path (str): æƒ…æ„Ÿè¯å…¸æ–‡ä»¶è·¯å¾„  
            stopwords_path (str): åœç”¨è¯æ–‡ä»¶è·¯å¾„
            bge_model_name (str): BGEæ¨¡å‹åç§°
        """
        self.numeric_data_path = Path(numeric_data_path)
        self.emotion_dict_path = Path(emotion_dict_path)
        self.stopwords_path = Path(stopwords_path)
        self.bge_model_name = bge_model_name
        
        # åˆå§‹åŒ–å·¥å…·ç»„ä»¶
        self.preprocessor = None
        self.vectorizer = None
        self.sentiment_calculator = None
        self.readability_calculator = None
        
        # æ•°æ®å­˜å‚¨
        self.numeric_data = None
        self.text_metrics_data = None
        
        # ç›¸ä¼¼åº¦è®¡ç®—çš„åŸºå‡†æ–‡æœ¬ï¼ˆç”¨äºå¤®è¡Œå’Œæ”¿åºœæ–‡æœ¬ï¼‰
        self.baseline_texts = {
            'central_bank': "å¤®è¡Œå®æ–½ç¨³å¥è´§å¸æ”¿ç­–ï¼Œä¿æŒæµåŠ¨æ€§åˆç†å……è£•ï¼Œæ”¯æŒå®ä½“ç»æµå‘å±•ã€‚",
            'government': "æ”¿åºœåšæŒé«˜è´¨é‡å‘å±•ï¼Œæ·±åŒ–ä¾›ç»™ä¾§ç»“æ„æ€§æ”¹é©ï¼Œæ¨è¿›ç»æµè½¬å‹å‡çº§ã€‚"
        }
        
        logger.info("æ–‡æœ¬æŒ‡æ ‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_components(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰è®¡ç®—ç»„ä»¶"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–è®¡ç®—ç»„ä»¶...")
            
            # åˆå§‹åŒ–æ–‡æœ¬é¢„å¤„ç†å™¨
            self.preprocessor = TextPreprocessor(str(self.stopwords_path))
            logger.info("âœ… æ–‡æœ¬é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–BGEå‘é‡åŒ–å™¨
            self.vectorizer = create_bge_vectorizer(
                model_name=self.bge_model_name,
                normalize_embeddings=True,
                use_instruction=False,
                preprocessor=self.preprocessor
            )
            if self.vectorizer is None:
                raise ValueError("BGEå‘é‡åŒ–å™¨åˆå§‹åŒ–å¤±è´¥")
            logger.info("âœ… BGEå‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æƒ…æ„Ÿè®¡ç®—å™¨
            self.sentiment_calculator = create_sentiment_calculator_from_file(
                str(self.emotion_dict_path), 
                self.preprocessor
            )
            if self.sentiment_calculator is None:
                raise ValueError("æƒ…æ„Ÿè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥")
            logger.info("âœ… æƒ…æ„Ÿè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–å¯è¯»æ€§è®¡ç®—å™¨
            self.readability_calculator = ReadabilityCalculator(self.preprocessor)
            logger.info("âœ… å¯è¯»æ€§è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
            
            logger.info("ğŸ‰ æ‰€æœ‰è®¡ç®—ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è®¡ç®—ç»„ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def load_numeric_data(self) -> pd.DataFrame:
        """åŠ è½½æ•°å€¼æ•°æ®
        
        Returns:
            pd.DataFrame: æ•°å€¼æ•°æ®
        """
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ•°å€¼æ•°æ®: {self.numeric_data_path}")
            
            if not self.numeric_data_path.exists():
                raise FileNotFoundError(f"æ•°å€¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.numeric_data_path}")
            
            # è¯»å–Excelæ–‡ä»¶
            self.numeric_data = pd.read_excel(self.numeric_data_path, engine='openpyxl')
            
            logger.info(f"âœ… æ•°å€¼æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.numeric_data)} è¡Œ")
            logger.info(f"æ•°æ®åˆ—å: {list(self.numeric_data.columns)}")
            
            return self.numeric_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°å€¼æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def calculate_text_metrics(self, text: str, baseline_text: Optional[str] = None) -> Dict[str, float]:
        """è®¡ç®—å•ä¸ªæ–‡æœ¬çš„æ‰€æœ‰æŒ‡æ ‡
        
        Args:
            text (str): å¾…åˆ†ææ–‡æœ¬
            baseline_text (Optional[str]): ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„åŸºå‡†æ–‡æœ¬
            
        Returns:
            Dict[str, float]: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        try:
            if not text or not text.strip():
                return {
                    'tone': 0.0,
                    'negative_tone': 0.0,
                    'similarity': 0.0,
                    'readability': 0.0
                }
            
            # è®¡ç®—æƒ…æ„ŸæŒ‡æ ‡
            sentiment_metrics = self.sentiment_calculator.calculate_all_metrics(text)
            tone = sentiment_metrics['tone']
            negative_tone = sentiment_metrics['negative_tone']
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            if baseline_text:
                similarity = SimilarityCalculator.text_similarity(
                    text, baseline_text, self.vectorizer
                )
            else:
                similarity = 0.0
            
            # è®¡ç®—å¯è¯»æ€§ï¼ˆä½¿ç”¨å­—ç¬¦çº§åˆ«ï¼‰
            readability = self.readability_calculator.calculate_readability(
                text, use_character_count=True
            )
            
            return {
                'tone': tone,
                'negative_tone': negative_tone,
                'similarity': similarity,
                'readability': readability
            }
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡æœ¬æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                'tone': 0.0,
                'negative_tone': 0.0,
                'similarity': 0.0,
                'readability': 0.0
            }
    
    def process_central_bank_texts(self) -> Dict[int, Dict[str, float]]:
        """å¤„ç†å¤®è¡Œæ–‡æœ¬æ•°æ®
        
        Returns:
            Dict[int, Dict[str, float]]: ä»¥å¹´ä»½ä¸ºé”®çš„å¤®è¡Œæ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†å¤®è¡Œæ–‡æœ¬æ•°æ®...")
            
            central_bank_dir = Path("data/original_data/text_data/å¤®è¡Œæ–‡æœ¬")
            if not central_bank_dir.exists():
                logger.error(f"å¤®è¡Œæ–‡æœ¬ç›®å½•ä¸å­˜åœ¨: {central_bank_dir}")
                return {}
            
            # è·å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
            txt_files = list(central_bank_dir.glob("*.txt"))
            
            # è¿‡æ»¤2001-2020å¹´çš„æ–‡ä»¶
            valid_files = []
            for txt_file in txt_files:
                filename = txt_file.stem
                year_match = re.match(r'(\d{4})ç¬¬å››å­£åº¦', filename)
                if year_match:
                    year = int(year_match.group(1))
                    if 2001 <= year <= 2020:
                        valid_files.append((txt_file, year))
            
            if not valid_files:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¤®è¡Œæ–‡æœ¬æ–‡ä»¶")
                return {}
            
            central_bank_metrics = {}
            baseline_text = self.baseline_texts['central_bank']
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(valid_files, desc="å¤„ç†å¤®è¡Œæ–‡æœ¬", unit="æ–‡ä»¶") as pbar:
                for txt_file, year in pbar:
                    try:
                        pbar.set_postfix(year=year)
                        
                        # è¯»å–æ–‡æœ¬å†…å®¹
                        with open(txt_file, 'r', encoding='utf-8') as f:
                            text_content = f.read().strip()
                        
                        if not text_content:
                            logger.warning(f"å¤®è¡Œæ–‡æœ¬æ–‡ä»¶ä¸ºç©º: {txt_file}")
                            continue
                        
                        # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
                        metrics = self.calculate_text_metrics(text_content, baseline_text)
                        central_bank_metrics[year] = metrics
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†å¤®è¡Œæ–‡æœ¬æ–‡ä»¶ {txt_file} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ğŸ‰ å¤®è¡Œæ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(central_bank_metrics)} å¹´çš„æ•°æ®")
            return central_bank_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤®è¡Œæ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def process_government_texts(self) -> Dict[Tuple[str, int], Dict[str, float]]:
        """å¤„ç†æ”¿åºœæ–‡æœ¬æ•°æ®
        
        Returns:
            Dict[Tuple[str, int], Dict[str, float]]: ä»¥(çœä»½, å¹´ä»½)ä¸ºé”®çš„æ”¿åºœæ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†æ”¿åºœæ–‡æœ¬æ•°æ®...")
            
            gov_file_path = Path("data/original_data/text_data/2001-2022çœçº§å·¥ä½œæ”¿åºœæŠ¥å‘Š.xlsx")
            if not gov_file_path.exists():
                logger.error(f"æ”¿åºœæ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {gov_file_path}")
                return {}
            
            # è¯»å–æ”¿åºœæ–‡æœ¬æ•°æ®
            gov_data = pd.read_excel(gov_file_path, engine='openpyxl')
            logger.info(f"æ”¿åºœæ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(gov_data)} è¡Œ")
            
            # è¿‡æ»¤2001-2020å¹´çš„æ•°æ®
            valid_data = gov_data[(gov_data['ä¼šè®¡å¹´'] >= 2001) & (gov_data['ä¼šè®¡å¹´'] <= 2020)]
            valid_data = valid_data.dropna(subset=['çœä»½åç§°', 'ä¼šè®¡å¹´', 'æ”¿åºœæŠ¥å‘Š'])
            valid_data = valid_data[valid_data['æ”¿åºœæŠ¥å‘Š'].str.strip() != '']
            
            logger.info(f"æœ‰æ•ˆçš„æ”¿åºœæ–‡æœ¬æ•°æ®: {len(valid_data)} æ¡")
            
            government_metrics = {}
            baseline_text = self.baseline_texts['government']
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(valid_data.iterrows(), 
                     desc="å¤„ç†æ”¿åºœæ–‡æœ¬", 
                     unit="æ¡", 
                     total=len(valid_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        province = str(row['çœä»½åç§°']).strip()
                        year = int(row['ä¼šè®¡å¹´'])
                        text_content = str(row['æ”¿åºœæŠ¥å‘Š']).strip()
                        
                        pbar.set_postfix(province=province[:4], year=year)
                        
                        if not text_content or text_content == 'nan':
                            continue
                        
                        # è®¡ç®—æ–‡æœ¬æŒ‡æ ‡
                        metrics = self.calculate_text_metrics(text_content, baseline_text)
                        government_metrics[(province, year)] = metrics
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†æ”¿åºœæ–‡æœ¬ç¬¬ {idx + 1} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ğŸ‰ æ”¿åºœæ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(government_metrics)} æ¡æ•°æ®")
            return government_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ”¿åºœæ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def process_management_texts(self) -> Dict[Tuple[str, int], Dict[str, float]]:
        """å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
        
        è®¡ç®—ç®¡ç†å±‚ç›¸ä¼¼åº¦ï¼šåŒä¸€å…¬å¸æœ¬å¹´åº¦ä¸ä¸Šä¸€å¹´åº¦æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Returns:
            Dict[Tuple[str, int], Dict[str, float]]: ä»¥(è‚¡ç¥¨ä»£ç , å¹´ä»½)ä¸ºé”®çš„ç®¡ç†å±‚æ–‡æœ¬æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ•°æ®...")
            
            mgmt_file_path = Path("data/original_data/text_data/2001-2020å¹´ä¸­å›½ä¸Šå¸‚å…¬å¸.ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ.å¹´æŠ¥æ–‡æœ¬.xlsx")
            if not mgmt_file_path.exists():
                logger.error(f"ç®¡ç†å±‚æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {mgmt_file_path}")
                return {}
            
            # è¯»å–ç®¡ç†å±‚æ–‡æœ¬æ•°æ®
            mgmt_data = pd.read_excel(mgmt_file_path, engine='openpyxl')
            logger.info(f"ç®¡ç†å±‚æ–‡æœ¬æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(mgmt_data)} è¡Œ")
            
            # è¿‡æ»¤2001-2020å¹´çš„æ•°æ®
            valid_data = mgmt_data[(mgmt_data['ä¼šè®¡å¹´åº¦'] >= 2001) & (mgmt_data['ä¼šè®¡å¹´åº¦'] <= 2020)]
            valid_data = valid_data.dropna(subset=['è‚¡ç¥¨ä»£ç ', 'ä¼šè®¡å¹´åº¦', 'ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'])
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str).str.strip() != '']
            valid_data = valid_data[valid_data['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹'].astype(str) != 'nan']
            
            logger.info(f"æœ‰æ•ˆçš„ç®¡ç†å±‚æ–‡æœ¬æ•°æ®: {len(valid_data)} æ¡")
            
            # å…ˆæ•´ç†æ•°æ®ï¼ŒæŒ‰å…¬å¸å’Œå¹´ä»½ç»„ç»‡
            logger.info("æ­£åœ¨æ•´ç†æ–‡æœ¬æ•°æ®...")
            company_texts = {}  # {è‚¡ç¥¨ä»£ç : {å¹´ä»½: æ–‡æœ¬å†…å®¹}}
            
            with tqdm(valid_data.iterrows(), 
                     desc="æ•´ç†ç®¡ç†å±‚æ–‡æœ¬", 
                     unit="æ¡", 
                     total=len(valid_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        year = int(row['ä¼šè®¡å¹´åº¦'])
                        text_content = str(row['ç»è¥è®¨è®ºä¸åˆ†æå†…å®¹']).strip()
                        
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        if stock_code not in company_texts:
                            company_texts[stock_code] = {}
                        
                        company_texts[stock_code][year] = text_content
                        
                    except Exception as e:
                        logger.error(f"æ•´ç†ç®¡ç†å±‚æ–‡æœ¬ç¬¬ {idx + 1} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"æ•´ç†å®Œæˆï¼Œå…± {len(company_texts)} å®¶å…¬å¸çš„æ–‡æœ¬æ•°æ®")
            
            # è®¡ç®—æŒ‡æ ‡
            management_metrics = {}
            similarity_count = 0
            
            # è®¡ç®—æ€»çš„å¤„ç†æ¡ç›®æ•°
            total_items = sum(len(year_texts) for year_texts in company_texts.values())
            
            with tqdm(total=total_items, 
                     desc="è®¡ç®—ç®¡ç†å±‚æŒ‡æ ‡", 
                     unit="æ¡") as pbar:
                
                for stock_code, year_texts in company_texts.items():
                    try:
                        for year, current_text in year_texts.items():
                            pbar.set_postfix(code=stock_code[:6], year=year)
                            
                            # è®¡ç®—åŸºæœ¬çš„æƒ…æ„Ÿå’Œå¯è¯»æ€§æŒ‡æ ‡
                            sentiment_metrics = self.sentiment_calculator.calculate_all_metrics(current_text)
                            tone = sentiment_metrics['tone']
                            negative_tone = sentiment_metrics['negative_tone']
                            
                            readability = self.readability_calculator.calculate_readability(
                                current_text, use_character_count=True
                            )
                            
                            # è®¡ç®—ä¸å‰ä¸€å¹´çš„ç›¸ä¼¼åº¦
                            similarity = 0.0
                            previous_year = year - 1
                            
                            if previous_year in year_texts:
                                previous_text = year_texts[previous_year]
                                if previous_text and previous_text.strip():
                                    similarity = SimilarityCalculator.text_similarity(
                                        current_text, previous_text, self.vectorizer
                                    )
                                    if similarity > 0:
                                        similarity_count += 1
                            
                            # å­˜å‚¨æŒ‡æ ‡
                            management_metrics[(stock_code, year)] = {
                                'tone': tone,
                                'negative_tone': negative_tone,
                                'similarity': similarity,
                                'readability': readability
                            }
                            
                            pbar.update(1)
                            
                    except Exception as e:
                        logger.error(f"å¤„ç†å…¬å¸ {stock_code} ç®¡ç†å±‚æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        # ä»ç„¶æ›´æ–°è¿›åº¦æ¡
                        remaining_items = len(year_texts)
                        pbar.update(remaining_items)
                        continue
            
            logger.info(f"ğŸ‰ ç®¡ç†å±‚æ–‡æœ¬å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(management_metrics)} æ¡æ•°æ®")
            logger.info(f"æˆåŠŸè®¡ç®—ç›¸ä¼¼åº¦çš„è®°å½•æ•°: {similarity_count}/{len(management_metrics)}")
            
            return management_metrics
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç®¡ç†å±‚æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {}
    
    def normalize_province_name(self, province_name: str) -> str:
        """æ ‡å‡†åŒ–çœä»½åç§°
        
        Args:
            province_name (str): åŸå§‹çœä»½åç§°
            
        Returns:
            str: æ ‡å‡†åŒ–åçš„çœä»½åç§°
        """
        if not province_name:
            return ""
        
        province_name = province_name.strip()
        
        # å¦‚æœçœä»½åç§°ä¸ä»¥"çœ"ç»“å°¾ï¼Œä¸”ä¸æ˜¯ç›´è¾–å¸‚æˆ–ç‰¹åˆ«è¡Œæ”¿åŒºï¼Œåˆ™æ·»åŠ "çœ"
        special_regions = {'åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†', 'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾', 
                          'å†…è’™å¤', 'å¹¿è¥¿', 'è¥¿è—', 'å®å¤', 'æ–°ç–†'}
        
        if province_name not in special_regions and not province_name.endswith('çœ'):
            province_name += 'çœ'
        
        return province_name
    
    def merge_text_metrics_with_numeric_data(self, 
                                           central_bank_metrics: Dict[int, Dict[str, float]],
                                           government_metrics: Dict[Tuple[str, int], Dict[str, float]],
                                           management_metrics: Dict[Tuple[str, int], Dict[str, float]]) -> pd.DataFrame:
        """å°†æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®åˆå¹¶
        
        Args:
            central_bank_metrics: å¤®è¡Œæ–‡æœ¬æŒ‡æ ‡
            government_metrics: æ”¿åºœæ–‡æœ¬æŒ‡æ ‡  
            management_metrics: ç®¡ç†å±‚æ–‡æœ¬æŒ‡æ ‡
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        try:
            logger.info("å¼€å§‹åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®...")
            
            # å¤åˆ¶æ•°å€¼æ•°æ®
            merged_data = self.numeric_data.copy()
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½', 'æ‰€å±çœä»½', 'è‚¡ç¥¨ä»£ç ']
            missing_columns = [col for col in required_columns if col not in merged_data.columns]
            
            if missing_columns:
                logger.error(f"æ•°å€¼æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                logger.info(f"ç°æœ‰åˆ—å: {list(merged_data.columns)}")
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            
            # åˆå§‹åŒ–æ–‡æœ¬æŒ‡æ ‡åˆ—
            text_metric_columns = [
                # å¤®è¡ŒæŒ‡æ ‡
                'å¤®è¡Œ_å‡€è¯­è°ƒ', 'å¤®è¡Œ_è´Ÿè¯­è°ƒ', 'å¤®è¡Œ_ç›¸ä¼¼åº¦', 'å¤®è¡Œ_å¯è¯»æ€§',
                # æ”¿åºœæŒ‡æ ‡
                'æ”¿åºœ_å‡€è¯­è°ƒ', 'æ”¿åºœ_è´Ÿè¯­è°ƒ', 'æ”¿åºœ_ç›¸ä¼¼åº¦', 'æ”¿åºœ_å¯è¯»æ€§',
                # ç®¡ç†å±‚æŒ‡æ ‡
                'ç®¡ç†å±‚_å‡€è¯­è°ƒ', 'ç®¡ç†å±‚_è´Ÿè¯­è°ƒ', 'ç®¡ç†å±‚_ç›¸ä¼¼åº¦', 'ç®¡ç†å±‚_å¯è¯»æ€§'
            ]
            
            for col in text_metric_columns:
                merged_data[col] = np.nan
            
            # åˆå¹¶å¤®è¡ŒæŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶å¤®è¡ŒæŒ‡æ ‡...")
            central_bank_matched = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶å¤®è¡ŒæŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # ä»"ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½"æå–å¹´ä»½
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(year=year)
                        
                        if year in central_bank_metrics:
                            metrics = central_bank_metrics[year]
                            merged_data.loc[idx, 'å¤®è¡Œ_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.loc[idx, 'å¤®è¡Œ_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.loc[idx, 'å¤®è¡Œ_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.loc[idx, 'å¤®è¡Œ_å¯è¯»æ€§'] = metrics['readability']
                            central_bank_matched += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶å¤®è¡ŒæŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"å¤®è¡ŒæŒ‡æ ‡åŒ¹é…æˆåŠŸ: {central_bank_matched} æ¡è®°å½•")
            
            # åˆå¹¶æ”¿åºœæŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶æ”¿åºœæŒ‡æ ‡...")
            government_matched = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶æ”¿åºœæŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # è·å–çœä»½å’Œå¹´ä»½
                        province = str(row['æ‰€å±çœä»½']).strip()
                        
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(province=province[:4], year=year)
                        
                        # ä»çœä»½åç§°ä¸­ç§»é™¤"çœ"å­—è¿›è¡ŒåŒ¹é…
                        province_key = province.replace('çœ', '') if province.endswith('çœ') else province
                        
                        # æŸ¥æ‰¾åŒ¹é…çš„æ”¿åºœæŒ‡æ ‡
                        gov_key = (province_key, year)
                        if gov_key in government_metrics:
                            metrics = government_metrics[gov_key]
                            merged_data.loc[idx, 'æ”¿åºœ_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.loc[idx, 'æ”¿åºœ_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.loc[idx, 'æ”¿åºœ_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.loc[idx, 'æ”¿åºœ_å¯è¯»æ€§'] = metrics['readability']
                            government_matched += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶æ”¿åºœæŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"æ”¿åºœæŒ‡æ ‡åŒ¹é…æˆåŠŸ: {government_matched} æ¡è®°å½•")
            
            # åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡
            logger.info("æ­£åœ¨åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡...")
            management_matched = 0
            
            with tqdm(merged_data.iterrows(), 
                     desc="åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡", 
                     unit="è¡Œ", 
                     total=len(merged_data)) as pbar:
                
                for idx, row in pbar:
                    try:
                        # è·å–è‚¡ç¥¨ä»£ç å’Œå¹´ä»½
                        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
                        
                        if pd.isna(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½']):
                            continue
                        
                        year = int(row['ç»Ÿè®¡æˆªæ­¢æ—¥æœŸ_å¹´ä»½'])
                        pbar.set_postfix(code=stock_code[:6], year=year)
                        
                        # æŸ¥æ‰¾åŒ¹é…çš„ç®¡ç†å±‚æŒ‡æ ‡
                        mgmt_key = (stock_code, year)
                        if mgmt_key in management_metrics:
                            metrics = management_metrics[mgmt_key]
                            merged_data.loc[idx, 'ç®¡ç†å±‚_å‡€è¯­è°ƒ'] = metrics['tone']
                            merged_data.loc[idx, 'ç®¡ç†å±‚_è´Ÿè¯­è°ƒ'] = metrics['negative_tone']
                            merged_data.loc[idx, 'ç®¡ç†å±‚_ç›¸ä¼¼åº¦'] = metrics['similarity']
                            merged_data.loc[idx, 'ç®¡ç†å±‚_å¯è¯»æ€§'] = metrics['readability']
                            management_matched += 1
                            
                    except Exception as e:
                        logger.warning(f"åˆå¹¶ç®¡ç†å±‚æŒ‡æ ‡ç¬¬ {idx} è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            
            logger.info(f"ç®¡ç†å±‚æŒ‡æ ‡åŒ¹é…æˆåŠŸ: {management_matched} æ¡è®°å½•")
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            logger.info("åˆå¹¶ç»“æœç»Ÿè®¡:")
            for col in text_metric_columns:
                non_na_count = merged_data[col].notna().sum()
                total_count = len(merged_data)
                coverage = non_na_count / total_count * 100
                logger.info(f"{col}: {non_na_count}/{total_count} ({coverage:.1f}%)")
            
            # ç‰¹åˆ«ç»Ÿè®¡ç®¡ç†å±‚ç›¸ä¼¼åº¦
            mgmt_similarity_count = merged_data['ç®¡ç†å±‚_ç›¸ä¼¼åº¦'].notna().sum()
            mgmt_similarity_positive = (merged_data['ç®¡ç†å±‚_ç›¸ä¼¼åº¦'] > 0).sum()
            logger.info(f"ç®¡ç†å±‚ç›¸ä¼¼åº¦ç»Ÿè®¡: æ€»è®¡{mgmt_similarity_count}æ¡, å…¶ä¸­{mgmt_similarity_positive}æ¡ç›¸ä¼¼åº¦>0")
            
            logger.info("ğŸ‰ æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®åˆå¹¶å®Œæˆ")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def save_results(self, merged_data: pd.DataFrame, 
                    output_path: str = "data/processed_data/origin_with_textmetric.csv") -> None:
        """ä¿å­˜åˆå¹¶åçš„ç»“æœ
        
        Args:
            merged_data (pd.DataFrame): åˆå¹¶åçš„æ•°æ®
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_path}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            merged_data.to_csv(output_path, index=False, encoding='utf-8')
            
            logger.info(f"âœ… ç»“æœä¿å­˜å®Œæˆ")
            logger.info(f"æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
            logger.info(f"æ•°æ®å½¢çŠ¶: {merged_data.shape}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def run_complete_analysis(self) -> pd.DataFrame:
        """è¿è¡Œå®Œæ•´çš„æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹
        
        Returns:
            pd.DataFrame: åŒ…å«æ–‡æœ¬æŒ‡æ ‡çš„å®Œæ•´æ•°æ®
        """
        try:
            logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹")
            
            # 1. åˆå§‹åŒ–ç»„ä»¶
            self.initialize_components()
            
            # 2. åŠ è½½æ•°å€¼æ•°æ®
            self.load_numeric_data()
            
            # 3. å¤„ç†å„ç±»æ–‡æœ¬æ•°æ®
            central_bank_metrics = self.process_central_bank_texts()
            government_metrics = self.process_government_texts()
            management_metrics = self.process_management_texts()
            
            # 4. åˆå¹¶æ–‡æœ¬æŒ‡æ ‡ä¸æ•°å€¼æ•°æ®
            merged_data = self.merge_text_metrics_with_numeric_data(
                central_bank_metrics, government_metrics, management_metrics
            )
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(merged_data)
            
            logger.info("ğŸ‰ æ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹å®Œæˆï¼")
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ è¿è¡Œæ–‡æœ¬æŒ‡æ ‡åˆ†ææµç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise


def main():
    """ä¸»å‡½æ•° - ç”¨äºç›´æ¥è¿è¡Œæ­¤æ¨¡å—æ—¶çš„æµ‹è¯•"""
    try:
        calculator = TextMetricCalculator()
        result_data = calculator.run_complete_analysis()
        
        print(f"\nğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ:")
        print(f"æ•°æ®å½¢çŠ¶: {result_data.shape}")
        print(f"\nåˆ—å: {list(result_data.columns)}")
        
        # æ˜¾ç¤ºæ–‡æœ¬æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        text_columns = [col for col in result_data.columns 
                       if any(keyword in col for keyword in ['å¤®è¡Œ', 'æ”¿åºœ', 'ç®¡ç†å±‚'])]
        
        print(f"\næ–‡æœ¬æŒ‡æ ‡åˆ—ç»Ÿè®¡:")
        for col in text_columns:
            non_na_count = result_data[col].notna().sum()
            mean_val = result_data[col].mean()
            print(f"{col}: {non_na_count} æ¡æœ‰æ•ˆæ•°æ®, å‡å€¼: {mean_val:.4f}")
            
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()